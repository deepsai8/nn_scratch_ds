{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn, save, load\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270935330f894097b29051bbefe651e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d92fc2f77a544ad974c556c41bffa55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7badc17dadd64822bca29ba7a18867ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d498985392a4fe78ebd72bace954ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "train = datasets.MNIST(root = 'data', download = True, train = True, transform =ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## minibatch size = 32\n",
    "## input images are of shape 1x28x28, meaning 1 channel, 28 px high, 28 px wide\n",
    "## classes are 0 to 9, so 10 different classes\n",
    "dataset = DataLoader(train, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN class\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, (3,3)), # ip channels = 1, op channels = 32, meaning 32 filters, each of shape 3x3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*(28-6)*(28-6),10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instane of the above NN , loss and clf (classifier)\n",
    "clf = ImageClassifier().to('cpu')\n",
    "## lr = learning rate\n",
    "opt = Adam(clf.parameters(), lr = 1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.019194109365344048\n",
      "Epoch: 1, loss: 0.004327834118157625\n",
      "Epoch: 2, loss: 0.0008858221117407084\n",
      "Epoch: 3, loss: 0.00016197173681575805\n",
      "Epoch: 4, loss: 1.5965655620675534e-05\n",
      "Epoch: 5, loss: 0.00014409572759177536\n",
      "Epoch: 6, loss: 6.991453847149387e-05\n",
      "Epoch: 7, loss: 5.9416433941805735e-06\n",
      "Epoch: 8, loss: 1.3454365216603037e-05\n",
      "Epoch: 9, loss: 1.8148279195884243e-05\n"
     ]
    }
   ],
   "source": [
    "## training flow, \n",
    "## put it under the if __name__, if using a .py file\n",
    "# if __name__ == \"__main__\":\n",
    "for epoch in range(10):\n",
    "    for batch in dataset:\n",
    "        # unpack the data\n",
    "        X, y = batch\n",
    "        X, y = X.to('cpu'), y.to('cpu')\n",
    "        yhat = clf(X)\n",
    "        loss = loss_fn(yhat, y)\n",
    "\n",
    "        ## Apply backpropagation\n",
    "        # Zero out any existing gradient\n",
    "        opt.zero_grad()\n",
    "        # calculate new gradient\n",
    "        loss.backward()\n",
    "        # apply gradient back\n",
    "        opt.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as pt file\n",
    "with open('model_state.pt', 'wb') as f:\n",
    "    save(clf.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model from file\n",
    "with open('model_state.pt','rb') as f:\n",
    "    clf.load_state_dict(load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "# make predictions on unseen data\n",
    "img = Image.open('data/img_3.jpg')\n",
    "img_tensor = ToTensor()(img).unsqueeze(0).to('cpu')\n",
    "print(torch.argmax(clf(img_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
